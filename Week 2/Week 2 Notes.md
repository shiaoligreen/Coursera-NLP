**N-gram Language Models**

Applications include suggestions in messengers, spelling correction, machine translation, speech recognition, handwriting recognition.

Markov assumption
$p(w_i|w_1...w_{i-1}) = p(w_i|w_{i-n+1}...w_{i-1})$

For a bigram
$p(w)=p(w_1)p(w_2|w_1)...p(w_k|w_{k-1})$

Normalise separately for each sequence length.

**Perplexity**

Train n-gram models using log-likelihood maximization and estimates for parameters.

To find the best n for the n-gram language model, it depends on how much data you have. Generally 5-gram models work well for language modelling.

Extrinsic evaluation - quality of a downstream task.

Intrinsic evaluation - hold-out (text) perplexity.

Perplexity

$P = p(w_{\text{test}})^{-\frac{1}{N}}) = \frac{1}{\sqrt[N]{p(w_{\text{test}})}}$

**Smoothing**

To deal with zeros in the test data, add smoothing (pull some probability from frequent bigrams to infrequent ones).

Laplacian smoothing - add a constant k to all accounts.

Interpolation smoothing - have a mixture of various n-gram models.

Absolute discounting - subtract 0.75 from your training data.

Kneser-Ney smoothing - captures the diversity of contexts for the word, probability of word is proportional to this.

**Hidden Markov Models**

**x** $= x_1, ... , x_T$ sequence of words (input)

**y** $= y_1, ..., y_T$ sequence of tags (labels)

Determine most probable sequence of tags given the sentence

**y** $= \text{argmax}_yp(y | x) = \text{argmax}_yp(x, y)$

Use Markov assumption and output independence to factorise.

Hidden Markov Model

$$p(x, y) = p(x|y)p(y) \approx \Pi_{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})$$

Text generation


*   Generate a sequence of tags
*   Generate some words given current tags

HMM specified by:


1.   Set $S = s_1, s_2, ..., s_N$ of hidden states
2.   Start state $s_0$
3.   Matrix $A$ of transition probabilities: $a_{ij}=p(s_j|s_i)$
4.  Set $O$ of possible visible outcomes
5. Matrix $B$ o output probabilities: $b_{ik}=p(o_k|s_i)$

Training is through maximum likelihood estimate when supervised.

$a_{ij} = p(s_j|s_i) = \frac{\sum_{t=1}^T [y_{t-1}=s_i, y_t = s_j]}{\sum_{t=1}^T[y_t=s_i]}$

For unsupervised learing, use Baum-Welch algorithm. E-step: posterior probabilities for hidden variables. M-step: maximum likelihood updates for the parameters.

**Viterbi Algorithm**

The same output sentence can be generated by different sequences of hidden states.

Let $Q_{t,s}$ be the most probable sequence of hidden states of length $t$ that finishes in the state $s$ and generates $o_1, ..., o_t$. Let $q_{t,s}$ be the probaility of this sequence. Dynamically

$q_{t,s} = \text{max}_{s'} q_{t-1,s'} \times p(s|s') \times p(o_t|s)$

**Sequential Models for Named Entity Recognition**

Maximum Entropy Markov Model - different factorisation and discriminative.

$$p(y|x) = \Pi_{t=1}^T p(y_t|y_{t-1},x_t)$$

Conditional Random Field (linear chain) - only one normalisation constraint, undirected and discriminative.

$$p(y|x) = \frac{1}{Z(x)} \Pi_{t=1}^T \exp \left(\sum_{k=1}^K \theta_kf_k(y_t, y_{t-1},x_t)\right)$$

Need to generate features to feed into the CRF models.

**Neural Language Models**

Curse of dimensionality - phenomenum that arises when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings.

Distributed representations - rerpresent words with low-dimensional vectors/ Hope that similar words have similar vectors. Define probabilistic model of data.

$y = b + Wx + U\tanh(d+Hx)$

Log-bilinear language model - fewer parameters and non-linear activations, measures similarity between word and context.

$p(w_i|w_{i-n+1}, ...w_{i-1}) = \frac{\exp(\hat{r}^Tr_{w_i} + b_{w_i})}{\sum_{w \in V}\exp(\hat{r}^Tr_w+b_w)}$

**Recurrent Neural Networks**

$h_i = f(Wh_{i-1} + Vx_i + b)$

$y_i = Uh_i + \tilde{b}$

Architecture
* use the current state output
* apply a linear layer on top
* do softmax to get the probabilities

Beam search - keeps in mind several sequences, so at every step you'll have, for example, five base sequences with highest possibilities.

RNN language model has lower perplexity and word error rate than 5-gram model with Knesser-Ney smoothing.

Typical language model - LSTM and gradient clipping.

Semantic role labelling - uses BIO notation (beginning, inside, outside). Bi-directional LSTMs are useful for this.
